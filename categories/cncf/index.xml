<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cncf on Pytimer Blog</title>
    <link>https://pytimer.github.io/categories/cncf/</link>
    <description>Recent content in Cncf on Pytimer Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jul 2020 09:47:00 +0800</lastBuildDate>
    
	<atom:link href="https://pytimer.github.io/categories/cncf/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>在Windows上使用Kubectl连接集群</title>
      <link>https://pytimer.github.io/2020/07/%E5%9C%A8windows%E4%B8%8A%E4%BD%BF%E7%94%A8kubectl%E8%BF%9E%E6%8E%A5%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 20 Jul 2020 09:47:00 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2020/07/%E5%9C%A8windows%E4%B8%8A%E4%BD%BF%E7%94%A8kubectl%E8%BF%9E%E6%8E%A5%E9%9B%86%E7%BE%A4/</guid>
      <description>平时都在Linux上搭建并操作Kubernetes集群，但是有时候会使用Windows来进行开发或者日常工作，这个时候如果在登录某台Linux主机，通过kubectl命令行操作集群就显得不是很方便，Kubernetes本身也是提供了Windows的命令行，因此在Windows上安装并配置kubectl，用于操作远端的Kubernetes集群。下面就是相关步骤。
下载Windows版本kubectl 可以按照官方文档Install kubectl on Windows来下载Windows版本的kubectl命令行。
因集群为v1.17.2版本，因此这里直接下载指定版本的kubectl.exe。
curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.2/bin/windows/amd64/kubectl.exe
下载好后，配置Windows的PATH，在该环境变量值后增加F:\k8s\bin(该路径为kubectl.exe的目录)。
配置好后，打开Git Bash或其他的工具，输入kubectl.exe version --client，可以看到对应的kubectl版本。到这里kubectl就安装好了。下面需要配置kubectl来连接远端Kubernetes集群。
$ kubectl.exe version --client Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;17&amp;quot;, GitVersion:&amp;quot;v1.17.2&amp;quot;, GitCommit:&amp;quot;59603c6e503c87169aea6106f57b9f242f64df89&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2020-01-18T23:30:10Z&amp;quot;, GoVersion:&amp;quot;go1.13.5&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;windows/amd64&amp;quot;}  配置kubectl kubectl需要使用kubeconfig来连接远端集群，通常有三种方式来指定kubeconfig。
 ~/.kube/config，默认kubectl会使用该目录。 KUBECONFIG，使用该环境变量的文件。 --kubeconfig，每次执行命令指定kubeconfig文件。  这里采用第2种方式。因此在Windows上配置一个环境变量KUBECONFIG。设置KUBECONFIG=F:\k8s\bin\kubeconfig。
重新打开Git Bash，执行kubectl.exe get nodes，可以看到连接到了远端的Kubernetes集群。
$ kubectl.exe get nodes NAME STATUS ROLES AGE VERSION meta-k8s-234 Ready master 29m v1.17.2 meta-k8s-235 Ready &amp;lt;none&amp;gt; 28m v1.17.2 meta-k8s-236 Ready &amp;lt;none&amp;gt; 28m v1.17.2 meta-k8s-237 Ready &amp;lt;none&amp;gt; 28m v1.</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard Installation</title>
      <link>https://pytimer.github.io/2019/12/kubernetes-dashboard-installation/</link>
      <pubDate>Tue, 17 Dec 2019 17:14:45 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/12/kubernetes-dashboard-installation/</guid>
      <description>Dashboard为Kubernetes官方的一个webui，集合了所有命令行可以操作的资源，可以根据浏览器的语言自动进行语言的识别。通过webui，我们可以更直观的看到Kubernetes集群的运行情况。不过在安装Dashboard的过程中有一些坑，因此在这里整理成文档以便在今后方便查阅。
安装 Dashboard的安装相对还是简单的，参考官方的github就可以。
 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
 不过在这中间有一个坑，那就是Dashboard的webui默认是自动生成证书的，由于时间和名称的问题，会导致Chrome和IE等浏览器无法打开界面，这里需要我们自己制作证书处理。
开启NodePort kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: ports: - port: 443 targetPort: 8443 nodePort: 30003 selector: k8s-app: kubernetes-dashboard type: NodePort  使用kubectl apply的命令将该service应用在Kubernetes集群中，可以看到该service已经生效。
[root@k8s ~]# kubectl get svc -n kube-system kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.106.67.12 &amp;lt;none&amp;gt; 443:30003/TCP 40m  通过浏览器访问Dashboard，我们发现无法访问界面，会有类似x.x.x.x通常会使用加密技术来保护您的信息，Google Chrome此次尝试连接到x.x.x.x时，此网站发回了异常的错误凭据。...的字样，更换IE、360浏览器均无法访问，在机器上使用curl -k https://x.x.x.x:30003/命令测试，是可以通的。
出现以上现象是因为Dashboard默认为webui生成的证书时间是无效的，时间是过期的，因此需要解决下该问题，以通过浏览器来访问。
解决证书过期问题 [root@k8s ~]# openssl genrsa -out dashboard.</description>
    </item>
    
    <item>
      <title>Coredns简介</title>
      <link>https://pytimer.github.io/2019/06/coredns%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Thu, 06 Jun 2019 18:06:42 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/06/coredns%E7%AE%80%E4%BB%8B/</guid>
      <description>DNS Linux上通过/etc/resolv.conf文件可以配置DNS相关信息，该文件是resovler类库所使用的配置文件，每当通过域名访问其他主机时，该类库会将域名转换为对应的IP，然后才可以进行访问。
resolv.conf 配置 nameserver 该选项用来配置DNS服务器地址，可以指定多个DNS
domain 该选项用来指定本地的域名，没有配置search的情况下，search默认使用的为domain的值
search 用来指定多个域名，用空格分隔。当访问的域名无法被DNS服务器解析时，resolver会将该域名后加上search指定的值，重新请求DNS，直到被正确解析或者列表循环结束。
nslookup 默认的超时时间为15s左右。
没有配置nameserver 在没有配置任何DNS的情况下，解析域名失败，返回如下错误：
$ nslookup baidu.com ;; connection timed out; no servers could be reached  配置nameserver $ cat /etc/resolv.conf nameserver 8.8.8.8  $ nslookup baidu.com Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: Name: baidu.com Address: x.x.x.x Name: baidu.com Address: y.y.y.y  如果没有解析域名成功会有如下信息：
$ nslookup aaa.bbb.ccc Server: 8.8.8.8 Address: 8.8.8.8#53 ** server can&#39;t find aaa.bbb.ccc: NXDOMAIN  DNS常见记录类型  A：指定域名对应的IP地址</description>
    </item>
    
    <item>
      <title>Change Etcd cluster member ip</title>
      <link>https://pytimer.github.io/2019/05/change-etcd-cluster-member-ip/</link>
      <pubDate>Tue, 28 May 2019 13:35:48 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/05/change-etcd-cluster-member-ip/</guid>
      <description>背景 Etcd是用于共享配置和服务发现的分布式、满足一致性的KV系统，受到Zookeeper启发，该项目是由CoreOS公司发起。目前在各种云环境中应用广泛，几个比较流行的云项目都采用了Etcd，如CloudFoundry、Kubernetes、Docker。Etcd采用Raft协议进行主节点的选举，并把相应的成员信息存储在各成员的db中。更多关于Etcd的详细介绍，可以在网上或者官方看到，这里不进行相关描述。
由于Kubernetes采用Etcd作为后端数据存储，如果Etcd出现问题，会导致整个Kubernetes集群的数据不一致，严重的甚至会导致集群不可用，因此需要掌握Etcd集群的常见问题解决方式，以便在该组件出现故障的时候，可以及时修复，从而保证Kubernetes集群的正常可用，不影响用户的使用。
如果Kubernetes集群部署完成后，更新整个集群所有节点的IP地址，当前Kubernetes的控制节点和Etcd成员节点在同一主机上运行，这也意味着如果修改Kubernetes控制节点的IP地址，需要对Etcd集群进行操作，以便Etcd集群可以使用新的IP地址进行通信。因此本文档重点介绍Etcd集群成员变更的两种方式。
成员变更解决方式 Etcd集群本身满足(n/2)+1的成员容忍性。下面为集群大小对应的可容忍的异常成员数量。
   集群大小 Majority 容忍数量     1 1 0   2 2 0   3 2 1   4 3 1   5 3 2   6 4 2   7 4 3   8 5 3   9 5 4    目前我们的Kubernetes集群采用大多数是3节点的Etcd集群，因此允许一个节点失联。
下面详细描述下如何进行集群成员IP变更，因为我们的Etcd集群运行在容器中，采用Static Pod的方式由Kubelet进行管理，所以如果Etcd集群部署形态并非容器的话，请根据实际情况进行相应调整，关于etcdctl的操作是一致的，没有区别。
我们仍然使用下面的三节点集群，所有节点IP地址如下：
   成员名称 旧IP 新IP     master1 192.</description>
    </item>
    
    <item>
      <title>预留计算资源</title>
      <link>https://pytimer.github.io/2019/05/%E9%A2%84%E7%95%99%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/</link>
      <pubDate>Mon, 13 May 2019 15:46:02 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/05/%E9%A2%84%E7%95%99%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/</guid>
      <description>Kubernetes Node Allocatable 表示整个集群当前节点pod可用的计算资源量，该配置可以保证调度器不会超额申请计算资源，目前支持CPU,memory,storage这几个参数。
Node Capacity --------------------------- | kube-reserved | |-------------------------| | system-reserved | |-------------------------| | eviction-threshold | |-------------------------| | | | allocatable | | (available for pods) | | | | | ---------------------------  https://k8smeetup.github.io/docs/tasks/administer-cluster/out-of-resource/#eviction-policy https://yq.aliyun.com/articles/604524 https://github.com/rootsongjc/qa/issues/3 https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#general-guidelines https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md#recommended-cgroups-setup https://github.com/rancher/rancher/issues/17177 https://github.com/kubernetes/kubernetes/blob/v1.11.6/pkg/kubelet/cm/cgroup_manager_linux.go#L259 http://www.kbase101.com/question/18764.html https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_control_groups#sec-Creating_Cgroups https://k8smeetup.github.io/docs/tasks/administer-cluster/reserve-compute-resources/ https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration</description>
    </item>
    
    <item>
      <title>[FAQ] Coredns启动失败</title>
      <link>https://pytimer.github.io/2019/03/faq-coredns%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/</link>
      <pubDate>Tue, 26 Mar 2019 22:09:07 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/03/faq-coredns%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/</guid>
      <description>coredns pod status is CrashLoopBackOff $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-cl9rp 0/1 CrashLoopBackOff 6 10m coredns-fb8b8dccf-v4lsv 0/1 CrashLoopBackOff 6 10m etcd-pytimer 1/1 Running 0 9m24s kube-apiserver-pytimer 1/1 Running 0 9m21s kube-controller-manager-pytimer 1/1 Running 0 9m24s kube-flannel-ds-amd64-84vxg 1/1 Running 0 8m14s kube-proxy-cgqpn 1/1 Running 0 10m kube-scheduler-pytimer 1/1 Running 0 9m15s  查看coredns pod的日志，发现如下错误：
$ kubectl logs -n kube-system coredns-fb8b8dccf-cl9rp .:53 2019-03-26T13:58:34.693Z [INFO] CoreDNS-1.3.1 2019-03-26T13:58:34.</description>
    </item>
    
    <item>
      <title>Deploy Helm private charts repository on the Kubernetes</title>
      <link>https://pytimer.github.io/2019/01/deploy-helm-private-charts-repository-on-the-kubernetes/</link>
      <pubDate>Tue, 08 Jan 2019 19:21:56 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/01/deploy-helm-private-charts-repository-on-the-kubernetes/</guid>
      <description>Helm是一个管理Kubernetes上charts的工具，通过Helm你可以比较方便的部署、卸载组件等操作。
在大多数时候，我们都可以直接使用官方的charts仓库来安装我们想要的组件，但是我们有时候会在内网的情况下使用Helm，这个时候我们可能需要一个自己私有的charts仓库。该文档将介绍如何搭建一个私有的charts仓库。
我们使用minio作为charts仓库的后端存储，结合chartmuseum来提供charts仓库的服务，以方便我们操作charts仓库。
术语  minio: minio是一个类似Amazon S3的分布式存储服务，该服务提供API、SDK、Client来让我们操作。 chartmuseum: chartmuseum是一个Web服务，该服务提供API来让我们比较轻松的操作charts仓库，而且该服务还可以满足多租户场景。  组件 部署完成后，全部的组件。
$ kubectl get pod -n kube-system -l &amp;quot;name=tiller&amp;quot; NAME READY STATUS RESTARTS AGE tiller-deploy-&amp;lt;xxx&amp;gt; 1/1 Running 0 5h $ kubectl get pod -n kube-system -l &amp;quot;app=chartmuseum&amp;quot; NAME READY STATUS RESTARTS AGE &amp;lt;release-name&amp;gt;-chartmuseum-&amp;lt;xxx&amp;gt; 1/1 Running 0 47m $ kubectl get pod -n storage NAME READY STATUS RESTARTS AGE minio-client-deployment-&amp;lt;xxx&amp;gt; 1/1 Running 0 7h minio-deployment-&amp;lt;xxx&amp;gt; 1/1 Running 0 9h  部署 部署私有charts仓库，初始化helm init，部署tiller服务，部署好minio服务，部署chartmuseum，通过helm install安装上述两个组件，启动服务。等待服务启动成功，使用helm添加新的charts仓库即可使用。</description>
    </item>
    
    <item>
      <title>Kube Admission</title>
      <link>https://pytimer.github.io/2018/12/kube-admission/</link>
      <pubDate>Mon, 17 Dec 2018 09:51:57 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/kube-admission/</guid>
      <description>External Admission Webhooks 作用及使用场景 当需要对某些api请求或者所有请求进行校验或者修改object的时候，可以考虑使用ValidatingAdmissionWebhook或者MutatingAdmissionWebhook，两者的区别：
 ValidatingAdmissionWebhook不允许在webhook中对Object进行修改，只是返回结果是true或false MutatingAdmissionWebhook运行在webhook中对Object进行修改  启用 在1.10之前的版本，需要使用 --admission-control 启用相关配置，并且是按配置的顺序来决定运行顺序，因此需要用户对于Admission Controllers 完全了解。
在1.10之后的版本，上述配置已经废弃，建议使用--enable-admission-plugins=MutatingAdmissionWebhook,ValidatingAdmissionWebhook，并且用户指定的顺序并不会影响实际运行顺序，更加友好。
官方的Using Admission Controllers - Kubernetes有关于这方面的详细说明。
流程 kube-apiserver &amp;ndash;&amp;gt; 认证鉴权 &amp;ndash;&amp;gt; Admission Controller &amp;ndash;&amp;gt; webhook
使用 创建admission服务，以供kube-apiserver调用 package main import ( &amp;quot;crypto/tls&amp;quot; &amp;quot;encoding/json&amp;quot; &amp;quot;flag&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;net/http&amp;quot; admissionv1beta1 &amp;quot;k8s.io/api/admission/v1beta1&amp;quot; admissionregistrationv1beta1 &amp;quot;k8s.io/api/admissionregistration/v1beta1&amp;quot; corev1 &amp;quot;k8s.io/api/core/v1&amp;quot; metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/runtime&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/runtime/serializer&amp;quot; utilruntime &amp;quot;k8s.io/apimachinery/pkg/util/runtime&amp;quot; &amp;quot;k8s.io/klog&amp;quot; ) type patchOperation struct { Op string `json:&amp;quot;op&amp;quot;` Path string `json:&amp;quot;path&amp;quot;` Value interface{} `json:&amp;quot;value,omitempty&amp;quot;` } var scheme = runtime.</description>
    </item>
    
    <item>
      <title>Kubeadm 设计</title>
      <link>https://pytimer.github.io/2018/12/kubeadm-%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Wed, 05 Dec 2018 11:26:10 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/kubeadm-%E8%AE%BE%E8%AE%A1/</guid>
      <description>Master：控制面节点
Node：工作节点
目标  构建高可用的Kubernetes集群。
 定义一种通用并且可扩展的多个控制面实例部署的流程。kubeadm join --control-plane。 为明确的用户用例提供一种更好的解决方案。  高级工具集成
  因为要提供给更高级的工具（ansible、saltstack）使用kubeadm来部署集群，因此，kubeadm join --control-plane需要为其提供更好的易操作性：
 并发部署节点
高级工具可以并行的创建节点（包括：控制面节点和工作节点），以减少集群的启动时间。kubeadm join --control-plane应该提供一种更好的实践方式而不是依赖于高级工具进行同步。
 支持动态和静态两种形态的部署流程
 在用户执行kubeadm init的时候，可能并不清楚集群最终的形态是什么样子，例如， 用户可能只启动了一个控制面节点和n个工作节点，然后在未来可能需要添加更多的控制面节点和工作节点。用户不能提前知道控制面节点的最终状态，这种流程叫做“动态部署”。
 kubeadm同时也应该支持另外一种“静态部署”流程，即用户在事先就已经规划并知道控制面节点的数量、IP等信息。
  支持不同的etcd部署场景，具体来说就是，在相同的节点上部署控制面组件和etcd，或者是在专有机器上运行etcd。
  非核心目标  在一个工作节点上安装控制面组件。该节点在最开始就要决定是作为控制面还是工作节点，并且在后续整个生命周期的过程中，都要保持不变。 该设计提案不包括etcd集群部署的管理。 该设计提案不包括api-server的load balancing。但是不能该提案的任何设计都不能阻止用户使用自己的负载均衡方案 该设计提案不涉及kubeadm的self-hosting方案。但是设计不能阻止未来重新考虑该部分。 该提案不提供对于跨主机传输CA和其他必要证书。 该提案不能阻塞当前已经存在的设计。  实现细节 初始化 Kubernetes 集群 在第一个节点上运行kubeadm init初始化集群，该节点被称为自举控制节点。
为支持kubeadm join --control-plane，新的Kubernetes集群必须要满足下面条件：
 集群必须设置一个controlPlaneEndpoint 集群必须使用外部etcd  执行join之前的准备 在调用kubeadm join --control-plane之前，需要用户或者使用高级工具拷贝自举控制节点的证书到新的节点。
证书包括：ca, front-proxy-ca certificate 和 service account key pair</description>
    </item>
    
    <item>
      <title>Install Kubernetes cluster with Kubeadm</title>
      <link>https://pytimer.github.io/2018/12/install-kubernetes-cluster-with-kubeadm/</link>
      <pubDate>Mon, 03 Dec 2018 16:58:17 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/install-kubernetes-cluster-with-kubeadm/</guid>
      <description>安装1.13.0-beta.2 安装必要组件 init apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token ttl: 24h0m0s usages: - signing - authentication localAPIEndpoint: advertiseAddress: 0.0.0.0 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: master212 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &amp;quot;${VIP}&amp;quot; controllerManager: {} dns: type: CoreDNS etcd: local: serverCertSANs: - &amp;quot;${VIP}&amp;quot; extraArgs: cipher-suites: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kubernetesVersion: v1.13.0-beta.2 networking: dnsDomain: cluster.</description>
    </item>
    
    <item>
      <title>Promethues Operator</title>
      <link>https://pytimer.github.io/2018/10/promethues-operator/</link>
      <pubDate>Sun, 28 Oct 2018 15:40:49 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/10/promethues-operator/</guid>
      <description>pormetheus-operator禁用cadvisor-port的方式 目前kubelet已经默认禁用--cadvisor-port的配置，而且后续该配置也会完全移除，因此需要改变prometheus-operator的serviceMonitor，修改的方式如下：
原有的配置方式：
- port: http-metrics interval: 15s - port: cadvisor interval: 30s honorLabels: true  修改后的配置方式：
- port: http-metrics interval: 15s - port: http-metrics path: /metrics/cadvisor interval: 30s honorLabels: true  参考的网址 https://github.com/coreos/prometheus-operator/issues/1400
https://github.com/coreos/prometheus-operator/issues/1741
https://github.com/coreos/prometheus-operator/issues/633</description>
    </item>
    
    <item>
      <title>Cloud Native</title>
      <link>https://pytimer.github.io/2018/06/cloud-native/</link>
      <pubDate>Fri, 01 Jun 2018 14:38:01 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/06/cloud-native/</guid>
      <description>参考
Kubernetes与云原生2017年年终总结与2018年展望</description>
    </item>
    
  </channel>
</rss>