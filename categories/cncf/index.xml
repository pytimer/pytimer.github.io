<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CNCF on Pytimer</title>
    <link>https://pytimer.github.io/categories/cncf/</link>
    <description>Recent content in CNCF on Pytimer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Mar 2021 13:36:21 +0800</lastBuildDate><atom:link href="https://pytimer.github.io/categories/cncf/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>修改Kubernetes集群节点的CIDR</title>
      <link>https://pytimer.github.io/post/cncf/kubernetes/change-cidr/</link>
      <pubDate>Mon, 22 Mar 2021 13:36:21 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/kubernetes/change-cidr/</guid>
      <description>使用Kubernetes集群的时候，遇到了在300台节点的时候，CNI从calico切换到flannel后，出现新建的pod无法分配IP的情况。
排查发现，是因为节点的podCIDR没有可分配的IP地址段。节点的podCIDR是由kube-controller-manager进行分配的，因此需要查看kube-controller-manager中--cluster-cidr和--node-cidr-mask-size配置，计算下支持的最大节点数。
集群最大节点数计算公式：(cluster-cidr的可用地址数)/(node-cidr-mask-size每台节点可用地址数)。
了解到集群修改过两次--cluster-cidr和--node-cidr-mask-size的值，集群刚创建的时候，使用的是--cluster-cidr=10.244.0.0/16和--node-cidr-mask-size=24，根据公式算出最大节点数为258，第二次修改配置为--cluster-cidr=10.244.0.0/12和--node-cidr-mask-size=16，根据公式算出最大节点数为16，因此随着节点规模增加，kube-controller-manager经过计算没有可分配的IP地址段给节点，所以节点的podCIDR为空。
原因了解后，修改kube-controller-manager的配置，即：--cluster-cidr=10.244.0.0/12和--node-cidr-mask-size=24，修改后集群可支持500以上节点。
修改后可以通过kubectl get nodes -ojsonpath=&amp;quot;{range .items[*]}{.metadata.name}{&#39;\t&#39;}{.spec.podCIDR}{&#39;\n&#39;}{end}&amp;quot;，看到所有节点都分配了IP地址段。
在节点已分配好podCIDR后，创建几个nginx的pod，确认是否解决问题。查看结果，可以看到IP已经可以分配，pod已经正常运行。
$ kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE nginx-deployment-6f769989f-2479j 1/1 Running 0 9d 10.244.52.4 worker1 nginx-deployment-6f769989f-278zs 1/1 Running 1 9d 10.240.31.10 worker289 nginx-deployment-6f769989f-27fv6 1/1 Running 1 9d 10.244.208.4 worker3 nginx-deployment-6f769989f-27phv 1/1 Running 1 9d 10.244.200.3 worker4 nginx-deployment-6f769989f-29hkt 1/1 Running 1 9d 10.241.12.6 worker5 虽然pod正常运行，但是发现在worker1上无法ping通worker289的pod，这个时候需要看下worker1的flannel的日志，看到日志有类似的错误：ignoring non-vxlan subnet(10.240.31.0/24): type=host-gw，因为我们的flannel采用的vxlan模式，而上面的错误，会导致flannel忽略worker289上的podCIDR，所以才会导致在worker1上无法连通worker289的pod。
连接不通的原因找到了，但是具体是什么导致了这个情况呢，查看了flannel的代码，没有找到比较明显的原因，只是怀疑可能是因为集群中有部分节点的podCIDR有重复的，所以导致flannel这边有问题，因此比对了下整个集群节点的podCIDR，发现果然有20个左右的节点重复，依次通过ping尝试连接这些节点的pod，都无法连通。到这里发现了根本原因，应该是之前两次修改--cluster-cidr和--node-cidr-mask-size导致的。因此下面要做的就是手动重新分配下重复的节点，使整个集群正常。
一开始想用kubectl patch修改podCIDR，但是发现不允许修改这个字段，因此只能采用另外一种方式，重新生成node的资源。因为在k8s上，所有的对象都是可以通过yaml来生成的，因此node也是可以通过yaml来重新生成。
$ kubectl get node &amp;lt;nodename&amp;gt; -oyaml &amp;gt; &amp;lt;nodename&amp;gt;.</description>
    </item>
    
    <item>
      <title>在Windows上使用Kubectl连接集群</title>
      <link>https://pytimer.github.io/post/cncf/kubernetes/kubectl/</link>
      <pubDate>Mon, 20 Jul 2020 09:47:00 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/kubernetes/kubectl/</guid>
      <description>平时都在Linux上搭建并操作Kubernetes集群，但是有时候会使用Windows来进行开发或者日常工作，这个时候如果在登录某台Linux主机，通过kubectl命令行操作集群就显得不是很方便，Kubernetes本身也是提供了Windows的命令行，因此在Windows上安装并配置kubectl，用于操作远端的Kubernetes集群。下面就是相关步骤。
下载Windows版本kubectl 可以按照官方文档Install kubectl on Windows来下载Windows版本的kubectl命令行。
因集群为v1.17.2版本，因此这里直接下载指定版本的kubectl.exe。
curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.2/bin/windows/amd64/kubectl.exe
下载好后，配置Windows的PATH，在该环境变量值后增加F:\k8s\bin(该路径为kubectl.exe的目录)。
配置好后，打开Git Bash或其他的工具，输入kubectl.exe version --client，可以看到对应的kubectl版本。到这里kubectl就安装好了。下面需要配置kubectl来连接远端Kubernetes集群。
$ kubectl.exe version --client Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;17&amp;#34;, GitVersion:&amp;#34;v1.17.2&amp;#34;, GitCommit:&amp;#34;59603c6e503c87169aea6106f57b9f242f64df89&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-01-18T23:30:10Z&amp;#34;, GoVersion:&amp;#34;go1.13.5&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;windows/amd64&amp;#34;} 配置kubectl kubectl需要使用kubeconfig来连接远端集群，通常有三种方式来指定kubeconfig。
 ~/.kube/config，默认kubectl会使用该目录。 KUBECONFIG，使用该环境变量的文件。 --kubeconfig，每次执行命令指定kubeconfig文件。  这里采用第2种方式。因此在Windows上配置一个环境变量KUBECONFIG。设置KUBECONFIG=F:\k8s\bin\kubeconfig。
重新打开Git Bash，执行kubectl.exe get nodes，可以看到连接到了远端的Kubernetes集群。
$ kubectl.exe get nodes NAME STATUS ROLES AGE VERSION meta-k8s-234 Ready master 29m v1.17.2 meta-k8s-235 Ready &amp;lt;none&amp;gt; 28m v1.17.2 meta-k8s-236 Ready &amp;lt;none&amp;gt; 28m v1.17.2 meta-k8s-237 Ready &amp;lt;none&amp;gt; 28m v1.</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard Installation</title>
      <link>https://pytimer.github.io/post/cncf/kubernetes/kubernetes-dashboard-installation/</link>
      <pubDate>Tue, 17 Dec 2019 17:14:45 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/kubernetes/kubernetes-dashboard-installation/</guid>
      <description>Dashboard为Kubernetes官方的一个webui，集合了所有命令行可以操作的资源，可以根据浏览器的语言自动进行语言的识别。通过webui，我们可以更直观的看到Kubernetes集群的运行情况。不过在安装Dashboard的过程中有一些坑，因此在这里整理成文档以便在今后方便查阅。
安装 Dashboard的安装相对还是简单的，参考官方的github就可以。
 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
 不过在这中间有一个坑，那就是Dashboard的webui默认是自动生成证书的，由于时间和名称的问题，会导致Chrome和IE等浏览器无法打开界面，这里需要我们自己制作证书处理。
开启NodePort kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: ports: - port: 443 targetPort: 8443 nodePort: 30003 selector: k8s-app: kubernetes-dashboard type: NodePort 使用kubectl apply的命令将该service应用在Kubernetes集群中，可以看到该service已经生效。
[root@k8s ~]# kubectl get svc -n kube-system kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.106.67.12 &amp;lt;none&amp;gt; 443:30003/TCP 40m 通过浏览器访问Dashboard，我们发现无法访问界面，会有类似x.x.x.x通常会使用加密技术来保护您的信息，Google Chrome此次尝试连接到x.x.x.x时，此网站发回了异常的错误凭据。...的字样，更换IE、360浏览器均无法访问，在机器上使用curl -k https://x.x.x.x:30003/命令测试，是可以通的。
出现以上现象是因为Dashboard默认为webui生成的证书时间是无效的，时间是过期的，因此需要解决下该问题，以通过浏览器来访问。
解决证书过期问题 [root@k8s ~]# openssl genrsa -out dashboard.</description>
    </item>
    
    <item>
      <title>Coredns简介</title>
      <link>https://pytimer.github.io/post/cncf/dns/coredns/</link>
      <pubDate>Thu, 06 Jun 2019 18:06:42 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/dns/coredns/</guid>
      <description>DNS Linux上通过/etc/resolv.conf文件可以配置DNS相关信息，该文件是resovler类库所使用的配置文件，每当通过域名访问其他主机时，该类库会将域名转换为对应的IP，然后才可以进行访问。
resolv.conf 配置 nameserver 该选项用来配置DNS服务器地址，可以指定多个DNS
domain 该选项用来指定本地的域名，没有配置search的情况下，search默认使用的为domain的值
search 用来指定多个域名，用空格分隔。当访问的域名无法被DNS服务器解析时，resolver会将该域名后加上search指定的值，重新请求DNS，直到被正确解析或者列表循环结束。
nslookup 默认的超时时间为15s左右。
没有配置nameserver 在没有配置任何DNS的情况下，解析域名失败，返回如下错误：
$ nslookup baidu.com ;; connection timed out; no servers could be reached 配置nameserver $ cat /etc/resolv.conf nameserver 8.8.8.8 $ nslookup baidu.com Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: Name: baidu.com Address: x.x.x.x Name: baidu.com Address: y.y.y.y 如果没有解析域名成功会有如下信息：
$ nslookup aaa.bbb.ccc Server: 8.8.8.8 Address: 8.8.8.8#53 ** server can&amp;#39;t find aaa.bbb.ccc: NXDOMAIN DNS常见记录类型   A：指定域名对应的IP地址
  AAAA：将域名解析到指定的IPv6的IP地址上</description>
    </item>
    
    <item>
      <title>Change Etcd cluster member ip</title>
      <link>https://pytimer.github.io/post/cncf/kubernetes/change-etcd-ip/</link>
      <pubDate>Tue, 28 May 2019 13:35:48 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/kubernetes/change-etcd-ip/</guid>
      <description>背景 Etcd是用于共享配置和服务发现的分布式、满足一致性的KV系统，受到Zookeeper启发，该项目是由CoreOS公司发起。目前在各种云环境中应用广泛，几个比较流行的云项目都采用了Etcd，如CloudFoundry、Kubernetes、Docker。Etcd采用Raft协议进行主节点的选举，并把相应的成员信息存储在各成员的db中。更多关于Etcd的详细介绍，可以在网上或者官方看到，这里不进行相关描述。
由于Kubernetes采用Etcd作为后端数据存储，如果Etcd出现问题，会导致整个Kubernetes集群的数据不一致，严重的甚至会导致集群不可用，因此需要掌握Etcd集群的常见问题解决方式，以便在该组件出现故障的时候，可以及时修复，从而保证Kubernetes集群的正常可用，不影响用户的使用。
如果Kubernetes集群部署完成后，更新整个集群所有节点的IP地址，当前Kubernetes的控制节点和Etcd成员节点在同一主机上运行，这也意味着如果修改Kubernetes控制节点的IP地址，需要对Etcd集群进行操作，以便Etcd集群可以使用新的IP地址进行通信。因此本文档重点介绍Etcd集群成员变更的两种方式。
成员变更解决方式 Etcd集群本身满足(n/2)+1的成员容忍性。下面为集群大小对应的可容忍的异常成员数量。
   集群大小 Majority 容忍数量     1 1 0   2 2 0   3 2 1   4 3 1   5 3 2   6 4 2   7 4 3   8 5 3   9 5 4    目前我们的Kubernetes集群采用大多数是3节点的Etcd集群，因此允许一个节点失联。
下面详细描述下如何进行集群成员IP变更，因为我们的Etcd集群运行在容器中，采用Static Pod的方式由Kubelet进行管理，所以如果Etcd集群部署形态并非容器的话，请根据实际情况进行相应调整，关于etcdctl的操作是一致的，没有区别。
我们仍然使用下面的三节点集群，所有节点IP地址如下：
   成员名称 旧IP 新IP     master1 192.</description>
    </item>
    
    <item>
      <title>Promethues Operator</title>
      <link>https://pytimer.github.io/post/cncf/prometheus/promethues-operator/</link>
      <pubDate>Sun, 28 Oct 2018 15:40:49 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/prometheus/promethues-operator/</guid>
      <description>pormetheus-operator禁用cadvisor-port的方式 目前kubelet已经默认禁用--cadvisor-port的配置，而且后续该配置也会完全移除，因此需要改变prometheus-operator的serviceMonitor，修改的方式如下：
原有的配置方式：
- port: http-metrics interval: 15s - port: cadvisor interval: 30s honorLabels: true 修改后的配置方式：
- port: http-metrics interval: 15s - port: http-metrics path: /metrics/cadvisor interval: 30s honorLabels: true 参考的网址 https://github.com/coreos/prometheus-operator/issues/1400
https://github.com/coreos/prometheus-operator/issues/1741
https://github.com/coreos/prometheus-operator/issues/633</description>
    </item>
    
    <item>
      <title>Cloud Native</title>
      <link>https://pytimer.github.io/post/cncf/cloud-native/</link>
      <pubDate>Fri, 01 Jun 2018 14:38:01 +0800</pubDate>
      
      <guid>https://pytimer.github.io/post/cncf/cloud-native/</guid>
      <description>参考
Kubernetes与云原生2017年年终总结与2018年展望</description>
    </item>
    
  </channel>
</rss>
