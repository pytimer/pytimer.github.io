<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud on pytimer</title>
    <link>https://pytimer.github.io/categories/cloud/</link>
    <description>Recent content in Cloud on pytimer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jan 2019 19:21:56 +0800</lastBuildDate>
    
	<atom:link href="https://pytimer.github.io/categories/cloud/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deploy Helm private charts repository on the Kubernetes</title>
      <link>https://pytimer.github.io/2019/01/deploy-helm-private-charts-repository-on-the-kubernetes/</link>
      <pubDate>Tue, 08 Jan 2019 19:21:56 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/01/deploy-helm-private-charts-repository-on-the-kubernetes/</guid>
      <description>Helm是一个管理Kubernetes上charts的工具，通过Helm你可以比较方便的部署、卸载组件等操作。
在大多数时候，我们都可以直接使用官方的charts仓库来安装我们想要的组件，但是我们有时候会在内网的情况下使用Helm，这个时候我们可能需要一个自己私有的charts仓库。该文档将介绍如何搭建一个私有的charts仓库。
我们使用minio作为charts仓库的后端存储，结合chartmuseum来提供charts仓库的服务，以方便我们操作charts仓库。
术语  minio: minio是一个类似Amazon S3的分布式存储服务，该服务提供API、SDK、Client来让我们操作。 chartmuseum: chartmuseum是一个Web服务，该服务提供API来让我们比较轻松的操作charts仓库，而且该服务还可以满足多租户场景。  组件 部署完成后，全部的组件。
$ kubectl get pod -n kube-system -l &amp;quot;name=tiller&amp;quot; NAME READY STATUS RESTARTS AGE tiller-deploy-&amp;lt;xxx&amp;gt; 1/1 Running 0 5h $ kubectl get pod -n kube-system -l &amp;quot;app=chartmuseum&amp;quot; NAME READY STATUS RESTARTS AGE &amp;lt;release-name&amp;gt;-chartmuseum-&amp;lt;xxx&amp;gt; 1/1 Running 0 47m $ kubectl get pod -n storage NAME READY STATUS RESTARTS AGE minio-client-deployment-&amp;lt;xxx&amp;gt; 1/1 Running 0 7h minio-deployment-&amp;lt;xxx&amp;gt; 1/1 Running 0 9h  部署 部署私有charts仓库，需要先部署minio服务，初始化helm init，部署tiller服务，这样一个初始的私有charts仓库就部署好了。在部署好初始私有charts仓库后，上传chartmuseum的charts，通过helm install安装该组件，启动服务。等待服务启动成功，使用helm添加新的charts仓库即可使用。</description>
    </item>
    
    <item>
      <title>Kube Admission</title>
      <link>https://pytimer.github.io/2018/12/kube-admission/</link>
      <pubDate>Mon, 17 Dec 2018 09:51:57 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/kube-admission/</guid>
      <description>External Admission Webhooks 作用及使用场景 当需要对某些api请求或者所有请求进行校验或者修改object的时候，可以考虑使用ValidatingAdmissionWebhook或者MutatingAdmissionWebhook，两者的区别：
 ValidatingAdmissionWebhook不允许在webhook中对Object进行修改，只是返回结果是true或false MutatingAdmissionWebhook运行在webhook中对Object进行修改  启用 在1.10之前的版本，需要使用 --admission-control 启用相关配置，并且是按配置的顺序来决定运行顺序，因此需要用户对于Admission Controllers 完全了解。
在1.10之后的版本，上述配置已经废弃，建议使用--enable-admission-plugins=MutatingAdmissionWebhook,ValidatingAdmissionWebhook，并且用户指定的顺序并不会影响实际运行顺序，更加友好。
官方的Using Admission Controllers - Kubernetes有关于这方面的详细说明。
流程 kube-apiserver &amp;ndash;&amp;gt; 认证鉴权 &amp;ndash;&amp;gt; Admission Controller &amp;ndash;&amp;gt; webhook
使用 创建admission服务，以供kube-apiserver调用 package main import ( &amp;quot;crypto/tls&amp;quot; &amp;quot;encoding/json&amp;quot; &amp;quot;flag&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;net/http&amp;quot; admissionv1beta1 &amp;quot;k8s.io/api/admission/v1beta1&amp;quot; admissionregistrationv1beta1 &amp;quot;k8s.io/api/admissionregistration/v1beta1&amp;quot; corev1 &amp;quot;k8s.io/api/core/v1&amp;quot; metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/runtime&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/runtime/serializer&amp;quot; utilruntime &amp;quot;k8s.io/apimachinery/pkg/util/runtime&amp;quot; &amp;quot;k8s.io/klog&amp;quot; ) type patchOperation struct { Op string `json:&amp;quot;op&amp;quot;` Path string `json:&amp;quot;path&amp;quot;` Value interface{} `json:&amp;quot;value,omitempty&amp;quot;` } var scheme = runtime.</description>
    </item>
    
    <item>
      <title>Kubeadm 设计</title>
      <link>https://pytimer.github.io/2018/12/kubeadm-%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Wed, 05 Dec 2018 11:26:10 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/kubeadm-%E8%AE%BE%E8%AE%A1/</guid>
      <description>Master：控制面节点
Node：工作节点
目标  构建高可用的Kubernetes集群。
 定义一种通用并且可扩展的多个控制面实例部署的流程。kubeadm join --control-plane。 为明确的用户用例提供一种更好的解决方案。  高级工具集成
  因为要提供给更高级的工具（ansible、saltstack）使用kubeadm来部署集群，因此，kubeadm join --control-plane需要为其提供更好的易操作性：
 并发部署节点
高级工具可以并行的创建节点（包括：控制面节点和工作节点），以减少集群的启动时间。kubeadm join --control-plane应该提供一种更好的实践方式而不是依赖于高级工具进行同步。
 支持动态和静态两种形态的部署流程
 在用户执行kubeadm init的时候，可能并不清楚集群最终的形态是什么样子，例如， 用户可能只启动了一个控制面节点和n个工作节点，然后在未来可能需要添加更多的控制面节点和工作节点。用户不能提前知道控制面节点的最终状态，这种流程叫做“动态部署”。
 kubeadm同时也应该支持另外一种“静态部署”流程，即用户在事先就已经规划并知道控制面节点的数量、IP等信息。
  支持不同的etcd部署场景，具体来说就是，在相同的节点上部署控制面组件和etcd，或者是在专有机器上运行etcd。
  非核心目标  在一个工作节点上安装控制面组件。该节点在最开始就要决定是作为控制面还是工作节点，并且在后续整个生命周期的过程中，都要保持不变。 该设计提案不包括etcd集群部署的管理。 该设计提案不包括api-server的load balancing。但是不能该提案的任何设计都不能阻止用户使用自己的负载均衡方案 该设计提案不涉及kubeadm的self-hosting方案。但是设计不能阻止未来重新考虑该部分。 该提案不提供对于跨主机传输CA和其他必要证书。 该提案不能阻塞当前已经存在的设计。  实现细节 初始化 Kubernetes 集群 在第一个节点上运行kubeadm init初始化集群，该节点被称为自举控制节点。
为支持kubeadm join --control-plane，新的Kubernetes集群必须要满足下面条件：
 集群必须设置一个controlPlaneEndpoint 集群必须使用外部etcd  执行join之前的准备 在调用kubeadm join --control-plane之前，需要用户或者使用高级工具拷贝自举控制节点的证书到新的节点。
证书包括：ca, front-proxy-ca certificate 和 service account key pair</description>
    </item>
    
    <item>
      <title>Install Kubernetes cluster with Kubeadm</title>
      <link>https://pytimer.github.io/2018/12/install-kubernetes-cluster-with-kubeadm/</link>
      <pubDate>Mon, 03 Dec 2018 16:58:17 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/install-kubernetes-cluster-with-kubeadm/</guid>
      <description>安装1.13.0-beta.2 安装必要组件 init apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token ttl: 24h0m0s usages: - signing - authentication localAPIEndpoint: advertiseAddress: 0.0.0.0 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: master212 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &amp;quot;${VIP}&amp;quot; controllerManager: {} dns: type: CoreDNS etcd: local: serverCertSANs: - &amp;quot;${VIP}&amp;quot; extraArgs: cipher-suites: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kubernetesVersion: v1.13.0-beta.2 networking: dnsDomain: cluster.</description>
    </item>
    
    <item>
      <title>Cloud Native</title>
      <link>https://pytimer.github.io/2018/06/cloud-native/</link>
      <pubDate>Fri, 01 Jun 2018 14:38:01 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/06/cloud-native/</guid>
      <description>参考
Kubernetes与云原生2017年年终总结与2018年展望</description>
    </item>
    
  </channel>
</rss>