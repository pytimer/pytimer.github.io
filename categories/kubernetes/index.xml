<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Pytimer Blog</title>
    <link>https://pytimer.github.io/categories/kubernetes/</link>
    <description>Recent content in Kubernetes on Pytimer Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2019 17:02:11 +0800</lastBuildDate>
    
	<atom:link href="https://pytimer.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[FAQ] 修改configmap后，使用subPath的Pod异常</title>
      <link>https://pytimer.github.io/2019/09/faq-%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E4%BD%BF%E7%94%A8subpath%E7%9A%84pod%E5%BC%82%E5%B8%B8/</link>
      <pubDate>Wed, 11 Sep 2019 17:02:11 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/09/faq-%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E4%BD%BF%E7%94%A8subpath%E7%9A%84pod%E5%BC%82%E5%B8%B8/</guid>
      <description>Pod状态异常，挂载configmap显示no such file or directory $ kubectl get pod -o wide test-7695dc7fb9-rjxlq 0/1 CrashLoopBackOff 13 17h 192.168.100.20 node1 &amp;lt;none&amp;gt;  CrashLoopBackOff状态需要查看pod的日志，通过kubectl logs可以查看到具体的错误信息。
$ kubectl logs -n test-7695dc7fb9-rjxlq container_linux.go:345: starting container process caused &amp;quot;process_linux.go:430: container init caused \&amp;quot;rootfs_linux.go:58: mounting \\\&amp;quot;/var/lib/kubelet/pods/ed1b799a-d3a5-11e9-add6-0894ef725f6e/volume-subpaths/test-cm/app-conf/1\\\&amp;quot; to rootfs \\\&amp;quot;/var/lib/docker/overlay/23a69e27f79f84a14b50fbee1e4840836487f8c740423291c2f785da1e7a6820/merged\\\&amp;quot; at \\\&amp;quot;/var/lib/docker/overlay/23a69e27f79f84a14b50fbee1e4840836487f8c740423291c2f785da1e7a6820/merged/etc/app/app.conf\\\&amp;quot; caused \\\&amp;quot;no such file or directory\\\&amp;quot;\&amp;quot;&amp;quot;  查看日志发现是挂载的configmap有问题。
猜测原因：该pod采用subPath的方式挂载configmap，怀疑是configmap修改后，未重建pod导致，因此会出现上述no such file or directory的错误。
排查问题 查看异常pod的uid，通过下述的命令我们可以看到当前pod在2019-09-10T08:35:21Z的时间创建，并且uid为ed1b799a-d3a5-11e9-add6-0894ef725f6e。
$ kubectl get pod test-7695dc7fb9-rjxlq -o json | jq .metadata { &amp;quot;creationTimestamp&amp;quot;: &amp;quot;2019-09-10T08:35:21Z&amp;quot;, &amp;quot;generateName&amp;quot;: &amp;quot;test-7695dc7fb9-&amp;quot;, &amp;quot;labels&amp;quot;: { &amp;quot;application&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;pod-template-hash&amp;quot;: &amp;quot;3251873965&amp;quot; }, &amp;quot;name&amp;quot;: &amp;quot;test-7695dc7fb9-rjxlq&amp;quot;, &amp;quot;namespace&amp;quot;: &amp;quot;openstack&amp;quot;, &amp;quot;ownerReferences&amp;quot;: [ { &amp;quot;apiVersion&amp;quot;: &amp;quot;apps/v1&amp;quot;, &amp;quot;blockOwnerDeletion&amp;quot;: true, &amp;quot;controller&amp;quot;: true, &amp;quot;kind&amp;quot;: &amp;quot;ReplicaSet&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;test-7695dc7fb9&amp;quot;, &amp;quot;uid&amp;quot;: &amp;quot;ece58508-d3a5-11e9-add6-0894ef725f6e&amp;quot; } ], &amp;quot;resourceVersion&amp;quot;: &amp;quot;1762165&amp;quot;, &amp;quot;selfLink&amp;quot;: &amp;quot;/api/v1/namespaces/default/pods/test-7695dc7fb9-rjxlq&amp;quot;, &amp;quot;uid&amp;quot;: &amp;quot;ed1b799a-d3a5-11e9-add6-0894ef725f6e&amp;quot; }  接下来就需要登录到pod所在的节点进行查看。</description>
    </item>
    
    <item>
      <title>Coredns简介</title>
      <link>https://pytimer.github.io/2019/06/coredns%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Thu, 06 Jun 2019 18:06:42 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/06/coredns%E7%AE%80%E4%BB%8B/</guid>
      <description>DNS Linux上通过/etc/resolv.conf文件可以配置DNS相关信息，该文件是resovler类库所使用的配置文件，每当通过域名访问其他主机时，该类库会将域名转换为对应的IP，然后才可以进行访问。
resolv.conf 配置 nameserver 该选项用来配置DNS服务器地址，可以指定多个DNS
domain 该选项用来指定本地的域名，没有配置search的情况下，search默认使用的为domain的值
search 用来指定多个域名，用空格分隔。当访问的域名无法被DNS服务器解析时，resolver会将该域名后加上search指定的值，重新请求DNS，直到被正确解析或者列表循环结束。
nslookup 默认的超时时间为15s左右。
没有配置nameserver 在没有配置任何DNS的情况下，解析域名失败，返回如下错误：
$ nslookup baidu.com ;; connection timed out; no servers could be reached  配置nameserver $ cat /etc/resolv.conf nameserver 8.8.8.8  $ nslookup baidu.com Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: Name: baidu.com Address: x.x.x.x Name: baidu.com Address: y.y.y.y  如果没有解析域名成功会有如下信息：
$ nslookup aaa.bbb.ccc Server: 8.8.8.8 Address: 8.8.8.8#53 ** server can&#39;t find aaa.bbb.ccc: NXDOMAIN  DNS常见记录类型  A：指定域名对应的IP地址</description>
    </item>
    
    <item>
      <title>Change Etcd cluster member ip</title>
      <link>https://pytimer.github.io/2019/05/change-etcd-cluster-member-ip/</link>
      <pubDate>Tue, 28 May 2019 13:35:48 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/05/change-etcd-cluster-member-ip/</guid>
      <description>背景 Etcd是用于共享配置和服务发现的分布式、满足一致性的KV系统，受到Zookeeper启发，该项目是由CoreOS公司发起。目前在各种云环境中应用广泛，几个比较流行的云项目都采用了Etcd，如CloudFoundry、Kubernetes、Docker。Etcd采用Raft协议进行主节点的选举，并把相应的成员信息存储在各成员的db中。更多关于Etcd的详细介绍，可以在网上或者官方看到，这里不进行相关描述。
由于Kubernetes采用Etcd作为后端数据存储，如果Etcd出现问题，会导致整个Kubernetes集群的数据不一致，严重的甚至会导致集群不可用，因此需要掌握Etcd集群的常见问题解决方式，以便在该组件出现故障的时候，可以及时修复，从而保证Kubernetes集群的正常可用，不影响用户的使用。
如果Kubernetes集群部署完成后，更新整个集群所有节点的IP地址，当前Kubernetes的控制节点和Etcd成员节点在同一主机上运行，这也意味着如果修改Kubernetes控制节点的IP地址，需要对Etcd集群进行操作，以便Etcd集群可以使用新的IP地址进行通信。因此本文档重点介绍Etcd集群成员变更的两种方式。
成员变更解决方式 Etcd集群本身满足(n/2)+1的成员容忍性。下面为集群大小对应的可容忍的异常成员数量。
   集群大小 Majority 容忍数量     1 1 0   2 2 0   3 2 1   4 3 1   5 3 2   6 4 2   7 4 3   8 5 3   9 5 4    目前我们的Kubernetes集群采用大多数是3节点的Etcd集群，因此允许一个节点失联。
下面详细描述下如何进行集群成员IP变更，因为我们的Etcd集群运行在容器中，采用Static Pod的方式由Kubelet进行管理，所以如果Etcd集群部署形态并非容器的话，请根据实际情况进行相应调整，关于etcdctl的操作是一致的，没有区别。
我们仍然使用下面的三节点集群，所有节点IP地址如下：
   成员名称 旧IP 新IP     master1 192.</description>
    </item>
    
    <item>
      <title>预留计算资源</title>
      <link>https://pytimer.github.io/2019/05/%E9%A2%84%E7%95%99%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/</link>
      <pubDate>Mon, 13 May 2019 15:46:02 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/05/%E9%A2%84%E7%95%99%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/</guid>
      <description>Kubernetes Node Allocatable 表示整个集群当前节点pod可用的计算资源量，该配置可以保证调度器不会超额申请计算资源，目前支持CPU,memory,storage这几个参数。
Node Capacity --------------------------- | kube-reserved | |-------------------------| | system-reserved | |-------------------------| | eviction-threshold | |-------------------------| | | | allocatable | | (available for pods) | | | | | ---------------------------  https://k8smeetup.github.io/docs/tasks/administer-cluster/out-of-resource/#eviction-policy https://yq.aliyun.com/articles/604524 https://github.com/rootsongjc/qa/issues/3 https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#general-guidelines https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md#recommended-cgroups-setup https://github.com/rancher/rancher/issues/17177 https://github.com/kubernetes/kubernetes/blob/v1.11.6/pkg/kubelet/cm/cgroup_manager_linux.go#L259 http://www.kbase101.com/question/18764.html https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_control_groups#sec-Creating_Cgroups https://k8smeetup.github.io/docs/tasks/administer-cluster/reserve-compute-resources/ https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration</description>
    </item>
    
    <item>
      <title>[FAQ] Coredns启动失败</title>
      <link>https://pytimer.github.io/2019/03/faq-coredns%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/</link>
      <pubDate>Tue, 26 Mar 2019 22:09:07 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/03/faq-coredns%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/</guid>
      <description>coredns pod status is CrashLoopBackOff $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-cl9rp 0/1 CrashLoopBackOff 6 10m coredns-fb8b8dccf-v4lsv 0/1 CrashLoopBackOff 6 10m etcd-pytimer 1/1 Running 0 9m24s kube-apiserver-pytimer 1/1 Running 0 9m21s kube-controller-manager-pytimer 1/1 Running 0 9m24s kube-flannel-ds-amd64-84vxg 1/1 Running 0 8m14s kube-proxy-cgqpn 1/1 Running 0 10m kube-scheduler-pytimer 1/1 Running 0 9m15s  查看coredns pod的日志，发现如下错误：
$ kubectl logs -n kube-system coredns-fb8b8dccf-cl9rp .:53 2019-03-26T13:58:34.693Z [INFO] CoreDNS-1.3.1 2019-03-26T13:58:34.</description>
    </item>
    
    <item>
      <title>Deploy Helm private charts repository on the Kubernetes</title>
      <link>https://pytimer.github.io/2019/01/deploy-helm-private-charts-repository-on-the-kubernetes/</link>
      <pubDate>Tue, 08 Jan 2019 19:21:56 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2019/01/deploy-helm-private-charts-repository-on-the-kubernetes/</guid>
      <description>Helm是一个管理Kubernetes上charts的工具，通过Helm你可以比较方便的部署、卸载组件等操作。
在大多数时候，我们都可以直接使用官方的charts仓库来安装我们想要的组件，但是我们有时候会在内网的情况下使用Helm，这个时候我们可能需要一个自己私有的charts仓库。该文档将介绍如何搭建一个私有的charts仓库。
我们使用minio作为charts仓库的后端存储，结合chartmuseum来提供charts仓库的服务，以方便我们操作charts仓库。
术语  minio: minio是一个类似Amazon S3的分布式存储服务，该服务提供API、SDK、Client来让我们操作。 chartmuseum: chartmuseum是一个Web服务，该服务提供API来让我们比较轻松的操作charts仓库，而且该服务还可以满足多租户场景。  组件 部署完成后，全部的组件。
$ kubectl get pod -n kube-system -l &amp;quot;name=tiller&amp;quot; NAME READY STATUS RESTARTS AGE tiller-deploy-&amp;lt;xxx&amp;gt; 1/1 Running 0 5h $ kubectl get pod -n kube-system -l &amp;quot;app=chartmuseum&amp;quot; NAME READY STATUS RESTARTS AGE &amp;lt;release-name&amp;gt;-chartmuseum-&amp;lt;xxx&amp;gt; 1/1 Running 0 47m $ kubectl get pod -n storage NAME READY STATUS RESTARTS AGE minio-client-deployment-&amp;lt;xxx&amp;gt; 1/1 Running 0 7h minio-deployment-&amp;lt;xxx&amp;gt; 1/1 Running 0 9h  部署 部署私有charts仓库，初始化helm init，部署tiller服务，部署好minio服务，部署chartmuseum，通过helm install安装上述两个组件，启动服务。等待服务启动成功，使用helm添加新的charts仓库即可使用。</description>
    </item>
    
    <item>
      <title>Kube Admission</title>
      <link>https://pytimer.github.io/2018/12/kube-admission/</link>
      <pubDate>Mon, 17 Dec 2018 09:51:57 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/kube-admission/</guid>
      <description>External Admission Webhooks 作用及使用场景 当需要对某些api请求或者所有请求进行校验或者修改object的时候，可以考虑使用ValidatingAdmissionWebhook或者MutatingAdmissionWebhook，两者的区别：
 ValidatingAdmissionWebhook不允许在webhook中对Object进行修改，只是返回结果是true或false MutatingAdmissionWebhook运行在webhook中对Object进行修改  启用 在1.10之前的版本，需要使用 --admission-control 启用相关配置，并且是按配置的顺序来决定运行顺序，因此需要用户对于Admission Controllers 完全了解。
在1.10之后的版本，上述配置已经废弃，建议使用--enable-admission-plugins=MutatingAdmissionWebhook,ValidatingAdmissionWebhook，并且用户指定的顺序并不会影响实际运行顺序，更加友好。
官方的Using Admission Controllers - Kubernetes有关于这方面的详细说明。
流程 kube-apiserver &amp;ndash;&amp;gt; 认证鉴权 &amp;ndash;&amp;gt; Admission Controller &amp;ndash;&amp;gt; webhook
使用 创建admission服务，以供kube-apiserver调用 package main import ( &amp;quot;crypto/tls&amp;quot; &amp;quot;encoding/json&amp;quot; &amp;quot;flag&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;net/http&amp;quot; admissionv1beta1 &amp;quot;k8s.io/api/admission/v1beta1&amp;quot; admissionregistrationv1beta1 &amp;quot;k8s.io/api/admissionregistration/v1beta1&amp;quot; corev1 &amp;quot;k8s.io/api/core/v1&amp;quot; metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/runtime&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/runtime/serializer&amp;quot; utilruntime &amp;quot;k8s.io/apimachinery/pkg/util/runtime&amp;quot; &amp;quot;k8s.io/klog&amp;quot; ) type patchOperation struct { Op string `json:&amp;quot;op&amp;quot;` Path string `json:&amp;quot;path&amp;quot;` Value interface{} `json:&amp;quot;value,omitempty&amp;quot;` } var scheme = runtime.</description>
    </item>
    
    <item>
      <title>Kubeadm 设计</title>
      <link>https://pytimer.github.io/2018/12/kubeadm-%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Wed, 05 Dec 2018 11:26:10 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/kubeadm-%E8%AE%BE%E8%AE%A1/</guid>
      <description>Master：控制面节点
Node：工作节点
目标  构建高可用的Kubernetes集群。
 定义一种通用并且可扩展的多个控制面实例部署的流程。kubeadm join --control-plane。 为明确的用户用例提供一种更好的解决方案。  高级工具集成
  因为要提供给更高级的工具（ansible、saltstack）使用kubeadm来部署集群，因此，kubeadm join --control-plane需要为其提供更好的易操作性：
 并发部署节点
高级工具可以并行的创建节点（包括：控制面节点和工作节点），以减少集群的启动时间。kubeadm join --control-plane应该提供一种更好的实践方式而不是依赖于高级工具进行同步。
 支持动态和静态两种形态的部署流程
 在用户执行kubeadm init的时候，可能并不清楚集群最终的形态是什么样子，例如， 用户可能只启动了一个控制面节点和n个工作节点，然后在未来可能需要添加更多的控制面节点和工作节点。用户不能提前知道控制面节点的最终状态，这种流程叫做“动态部署”。
 kubeadm同时也应该支持另外一种“静态部署”流程，即用户在事先就已经规划并知道控制面节点的数量、IP等信息。
  支持不同的etcd部署场景，具体来说就是，在相同的节点上部署控制面组件和etcd，或者是在专有机器上运行etcd。
  非核心目标  在一个工作节点上安装控制面组件。该节点在最开始就要决定是作为控制面还是工作节点，并且在后续整个生命周期的过程中，都要保持不变。 该设计提案不包括etcd集群部署的管理。 该设计提案不包括api-server的load balancing。但是不能该提案的任何设计都不能阻止用户使用自己的负载均衡方案 该设计提案不涉及kubeadm的self-hosting方案。但是设计不能阻止未来重新考虑该部分。 该提案不提供对于跨主机传输CA和其他必要证书。 该提案不能阻塞当前已经存在的设计。  实现细节 初始化 Kubernetes 集群 在第一个节点上运行kubeadm init初始化集群，该节点被称为自举控制节点。
为支持kubeadm join --control-plane，新的Kubernetes集群必须要满足下面条件：
 集群必须设置一个controlPlaneEndpoint 集群必须使用外部etcd  执行join之前的准备 在调用kubeadm join --control-plane之前，需要用户或者使用高级工具拷贝自举控制节点的证书到新的节点。
证书包括：ca, front-proxy-ca certificate 和 service account key pair</description>
    </item>
    
    <item>
      <title>Install Kubernetes cluster with Kubeadm</title>
      <link>https://pytimer.github.io/2018/12/install-kubernetes-cluster-with-kubeadm/</link>
      <pubDate>Mon, 03 Dec 2018 16:58:17 +0800</pubDate>
      
      <guid>https://pytimer.github.io/2018/12/install-kubernetes-cluster-with-kubeadm/</guid>
      <description>安装1.13.0-beta.2 安装必要组件 init apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token ttl: 24h0m0s usages: - signing - authentication localAPIEndpoint: advertiseAddress: 0.0.0.0 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: master212 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &amp;quot;${VIP}&amp;quot; controllerManager: {} dns: type: CoreDNS etcd: local: serverCertSANs: - &amp;quot;${VIP}&amp;quot; extraArgs: cipher-suites: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kubernetesVersion: v1.13.0-beta.2 networking: dnsDomain: cluster.</description>
    </item>
    
  </channel>
</rss>